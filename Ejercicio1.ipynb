{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ejercicio1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cUOT0TsOoDzz",
        "WxAHGanvpPnI",
        "ai2DT1HitKMn"
      ],
      "authorship_tag": "ABX9TyOH8ayqrUpjFNuGunkiOBvS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pablobermudez/SOA_EA3/blob/master/Ejercicio1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf1PpWeeTcoI"
      },
      "source": [
        "# 1 Introducción\n",
        "\n",
        "El ejercicio posee dos versiones del algoritmo de ordenamiento bubble sort, la primera de ellas deberá ejecutarse en el entorno CPU con el lenguaje python y la segunda en el entorno GPU utilizando el lenguaje cuda junto con la libreria pycuda.\n",
        "\n",
        "La finalidad del ejercicio no es centrarse en la complejidad del algoritmo utilizado, sino en la información que nos provee y en el sustento teórico que poseen.\n",
        "\n",
        "A priori, ya investigado el tema, sabemos que algoritmos como bubble sort no son algoritmos idoneos para ser ejecutados por el GPU. Esto es debido a que los hilos del kernel deberán sincronizarse obligatoriamente para poder realizar su tarea de manera coordinada y efectiva.\n",
        "\n",
        "Para entender mejor lo mencionado, ejecutaremos la prueba y veremos entonces las conclusiones.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUOT0TsOoDzz"
      },
      "source": [
        "#2 Armado del ambiente\n",
        "Instalar en el cuaderno el módulo CUDA de Python.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eceh5yt8oQ1y"
      },
      "source": [
        "!pip install pycuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxAHGanvpPnI"
      },
      "source": [
        "#3.1 Desarrollo - CPU\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "eTlmmqpspVZY"
      },
      "source": [
        "# --------------------------------------------\n",
        "#@title 3.1.1 Parámetros de ejecución { vertical-output: true }\n",
        "\n",
        "cantidad_N =   6000#@param {type: \"number\"}\n",
        "# --------------------------------------------\n",
        "\n",
        "# --------------------------------------------\n",
        "# Valido número de ingreso por el usuario\n",
        "if (cantidad_N == 0):\n",
        "  sys.exit(\"La cantidad de números en el vector debe ser mayor que 0\")\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "tiempo_total = datetime.now()\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# --------------------------------------------\n",
        "# Definición de función que transforma el tiempo en  milisegundos \n",
        "tiempo_en_ms = lambda dt:(dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0\n",
        "\n",
        "# --------------------------------------------\n",
        "# Defino variables\n",
        "arr = [random.randint(1,10) for _ in range(cantidad_N)]\n",
        "\n",
        "# Defino algoritmo\n",
        "def bubbleSort(arr): \n",
        "    n = len(arr) \n",
        "    for i in range(n-1): \n",
        "        for j in range(0, n-i-1): \n",
        "            if arr[j] > arr[j+1] : \n",
        "                arr[j], arr[j+1] = arr[j+1], arr[j] \n",
        "  \n",
        "bubbleSort(arr) \n",
        "\n",
        "\n",
        "# TODO - Informo tiempos, hilos y bloques.\n",
        "tiempo_total = datetime.now() - tiempo_total\n",
        "print(\"Tiempo CPU: \", tiempo_en_ms( tiempo_total ), \"[ms]\" )\n",
        "\n",
        "# CPU - Informo el resutlado.\n",
        "print( \"------------------------------------\")\n",
        "print( \"Array: \" )\n",
        "print( arr )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai2DT1HitKMn"
      },
      "source": [
        "#3.2 Desarrollo - GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ds_geZIJtLtI"
      },
      "source": [
        "# --------------------------------------------\n",
        "#@title 3.2.1 Parámetros de ejecución { vertical-output: true }\n",
        "\n",
        "cantidad_N =   300#@param {type: \"number\"}\n",
        "# --------------------------------------------\n",
        "\n",
        "from datetime import datetime\n",
        "tiempo_total = datetime.now()\n",
        "\n",
        "import sys\n",
        "import random\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "import numpy as np\n",
        "\n",
        "# --------------------------------------------\n",
        "# Valido número de ingreso por el usuario\n",
        "if (cantidad_N == 0):\n",
        "  sys.exit(\"La cantidad de números en el vector debe ser mayor que 0\")\n",
        "\n",
        "\n",
        "# --------------------------------------------\n",
        "# Definición de función que transforma el tiempo en  milisegundos \n",
        "tiempo_en_ms = lambda dt:(dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0\n",
        "\n",
        "# CPU - Defino la memoria de los vectores en cpu.\n",
        "x_cpu = [random.randint(1,10) for _ in range(cantidad_N)]\n",
        "x_cpu = np.array(x_cpu, dtype=np.int32)\n",
        "r_cpu = numpy.empty_like( x_cpu )\n",
        "\n",
        "# CPU - reservo la memoria GPU.\n",
        "x_gpu = cuda.mem_alloc( x_cpu.nbytes )\n",
        "\n",
        "# GPU - Copio la memoria al GPU.\n",
        "cuda.memcpy_htod( x_gpu, x_cpu )\n",
        "\n",
        "# CPU - Defino la función kernel que ejecutará en GPU.\n",
        "module = SourceModule(\"\"\"\n",
        "\n",
        "#include <stdio.h>\n",
        "__global__ void bubble_sort_gpu( int n, float *X)\n",
        "{\n",
        "\n",
        "   int i = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "   float aux;\n",
        "    for (int p = 0; p < n; ++p) {\n",
        "        if ((i - p) % 2 && i < n - 1 && X[i] > X[i + 1]) {\n",
        "            aux = X[i];\n",
        "            X[i] = X[i + 1];\n",
        "            X[i + 1] = aux;\n",
        "        }     \n",
        "        __syncthreads();\n",
        "    }\n",
        "}\n",
        "\n",
        "\"\"\") \n",
        "# CPU - Genero la función kernel.\n",
        "kernel = module.get_function(\"bubble_sort_gpu\")\n",
        "tiempo_gpu = datetime.now()\n",
        "\n",
        "# GPU - Ejecuta el kernel.\n",
        "# TODO: Falta consultar limites del GPU, para armar las dimensiones correctamente.\n",
        "dim_hilo = 256\n",
        "dim_bloque = numpy.int( (cantidad_N+dim_hilo-1) / dim_hilo )\n",
        "print( \"Thread x: \", dim_hilo, \", Bloque x:\", dim_bloque )\n",
        "\n",
        "#TODO: Ojo, con los tipos de las variables en el kernel.\n",
        "kernel( numpy.int32(cantidad_N), x_gpu, block=( dim_hilo, 1, 1 ),grid=(dim_bloque, 1,1) )\n",
        "\n",
        "tiempo_gpu = datetime.now() - tiempo_gpu\n",
        "\n",
        "# GPU - Copio el resultado desde la memoria GPU.\n",
        "cuda.memcpy_dtoh( r_cpu, x_gpu )\n",
        "\n",
        "tiempo_total = datetime.now() - tiempo_total\n",
        "\n",
        "# TODO - Informo tiempos, hilos y bloques.\n",
        "print( \"Cantidad de elementos: \", cantidad_N )\n",
        "print( \"Thread x: \", dim_hilo, \", Bloque x:\", dim_bloque )\n",
        "print(\"Tiempo CPU: \", tiempo_en_ms( tiempo_total ), \"[ms]\" )\n",
        "print(\"Tiempo GPU: \", tiempo_en_ms( tiempo_gpu   ), \"[ms]\" )\n",
        "\n",
        "# CPU - Informo el resutlado.\n",
        "print( \"------------------------------------\")\n",
        "print( \"X: \" )\n",
        "print( x_cpu )\n",
        "print( \"------------------------------------\")\n",
        "print( \"R: \" )\n",
        "print( r_cpu )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlJNPDXheHGn"
      },
      "source": [
        "#4 Tabla de Pasos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFfBjyXC1-70"
      },
      "source": [
        "# 6 Referencias\n",
        "\n",
        "\n",
        "*   https://dergipark.org.tr/en/download/article-file/225714\n",
        "* https://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__SINGLE.html#group__CUDA__MATH__SINGLE\n",
        "* https://documen.tician.de/pycuda/driver.html\n",
        "* https://arxiv.org/ftp/arxiv/papers/1505/1505.07605.pdf\n"
      ]
    }
  ]
}